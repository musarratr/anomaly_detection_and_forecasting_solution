{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a557a67-4d10-421e-96e8-2c951fbe9919",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Generic anomaly detection for minute-level Oxygen readings.\n",
    "\n",
    "Covers:\n",
    "- Point anomalies (outliers)\n",
    "- Collective anomalies (patterns / sequences)\n",
    "- Contextual anomalies (conditional on hour-of-day)\n",
    "- Sensor fault anomalies (stuck sensor, spikes/glitches, high noise)\n",
    "\n",
    "Produces:\n",
    "- Per-minute anomaly severity score in [0, 1]\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ec96c7-6b57-422e-aace-57fadc6799e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 0. Configuration\n",
    "# =========================\n",
    "\n",
    "@dataclass\n",
    "class AnomalyConfig:\n",
    "    # Rolling windows (in minutes)\n",
    "    roll_window_baseline: int = 60           # 1-hour baseline\n",
    "    roll_window_collective: int = 120        # 2-hour window for collective anomalies\n",
    "    roll_window_stuck: int = 60              # 1-hour rolling std for 'stuck'\n",
    "    roll_window_noise: int = 60              # 1-hour rolling std for 'noise'\n",
    "\n",
    "    # Point / contextual anomaly z-score thresholds for score scaling\n",
    "    z_point_low: float = 2.0                 # start increasing score\n",
    "    z_point_high: float = 5.0                # max severity for point anomalies\n",
    "    z_ctx_low: float = 2.0\n",
    "    z_ctx_high: float = 4.0\n",
    "\n",
    "    # Collective anomaly thresholds (mean |z| over a window)\n",
    "    collective_low: float = 1.0\n",
    "    collective_high: float = 3.0\n",
    "\n",
    "    # Sensor-fault parameters\n",
    "    stuck_rel_std_factor: float = 0.1        # local std < 0.1 * typical std ⇒ stuck\n",
    "    noise_factor: float = 2.0                # local std > 2 * typical std ⇒ noisy\n",
    "    spike_z_threshold: float = 3.0           # how many std-devs for spikes\n",
    "\n",
    "    # Small epsilon for numerical stability\n",
    "    eps: float = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd689c2-068c-438d-a86e-b522426c6c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 1. Loading & preparation\n",
    "# =========================\n",
    "\n",
    "def load_oxygen_data(csv_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load the raw oxygen CSV.\n",
    "    Assumes columns: time, Oxygen[%sat], EquipmentUnit, SubUnit, System, Unit.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    # Parse time\n",
    "    df[\"time\"] = pd.to_datetime(df[\"time\"], errors=\"coerce\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def prepare_sensor_frame(df_raw: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Prepare a generic sensor frame:\n",
    "\n",
    "    - Drop rows with missing Oxygen.\n",
    "    - Create a generic sensor_id that does NOT rely on tag semantics.\n",
    "    - Keep only (time, sensor_id, value, basic time features).\n",
    "    \"\"\"\n",
    "    df = df_raw.copy()\n",
    "\n",
    "    # Keep only rows where Oxygen is present\n",
    "    df = df[df['time'].notna() & df[\"Oxygen[%sat]\"].notna()].copy()\n",
    "\n",
    "    # Generic sensor id: we just use tags as opaque identifiers.\n",
    "    # This will still generalize to new customers & tag values.\n",
    "    df[\"sensor_id\"] = (\n",
    "        df[\"System\"].astype(str)\n",
    "        + \"|\"\n",
    "        + df[\"EquipmentUnit\"].astype(str)\n",
    "        + \"|\"\n",
    "        + df[\"SubUnit\"].astype(str)\n",
    "    )\n",
    "\n",
    "    df.rename(columns={\"Oxygen[%sat]\": \"oxygen\"}, inplace=True)\n",
    "\n",
    "    # Basic time features for contextual anomalies\n",
    "    df[\"hour\"] = df[\"time\"].dt.hour\n",
    "    df[\"dayofweek\"] = df[\"time\"].dt.dayofweek\n",
    "\n",
    "    # Sort by sensor and time to make rolling operations valid\n",
    "    df = df.sort_values([\"sensor_id\", \"time\"]).reset_index(drop=True)\n",
    "\n",
    "    return df[[\"time\", \"sensor_id\", \"oxygen\", \"hour\", \"dayofweek\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ac8326-e324-45cf-be7c-a84c78ad1c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 2. Helper: score scaling\n",
    "# =========================\n",
    "\n",
    "def squash_z(z: pd.Series, low: float, high: float) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Map |z| to [0, 1] linearly between `low` and `high`.\n",
    "\n",
    "    - |z| <= low  => 0\n",
    "    - |z| >= high => 1\n",
    "    - between     => linear ramp\n",
    "    \"\"\"\n",
    "    az = z.abs()\n",
    "    return ((az - low) / (high - low)).clip(lower=0.0, upper=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce584e0c-0607-412b-8370-0ebc5ba85d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 3. Baseline & residuals\n",
    "# =========================\n",
    "\n",
    "def add_baseline_and_point_scores(df: pd.DataFrame, cfg: AnomalyConfig) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    For each sensor_id:\n",
    "    - Compute rolling baseline (mean + std) over `roll_window_baseline`.\n",
    "    - Compute standardized residual z-score.\n",
    "    - Compute point anomaly score from z-score.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    g = df.groupby(\"sensor_id\", group_keys=False)\n",
    "\n",
    "    # Rolling mean & std as local baseline\n",
    "    df[\"roll_mean\"] = g[\"oxygen\"].transform(\n",
    "        lambda s: s.rolling(\n",
    "            window=cfg.roll_window_baseline,\n",
    "            min_periods=cfg.roll_window_baseline // 2\n",
    "        ).mean()\n",
    "    )\n",
    "    df[\"roll_std\"] = g[\"oxygen\"].transform(\n",
    "        lambda s: s.rolling(\n",
    "            window=cfg.roll_window_baseline,\n",
    "            min_periods=cfg.roll_window_baseline // 2\n",
    "        ).std()\n",
    "    )\n",
    "\n",
    "    # Standardized residual (z-score vs rolling baseline)\n",
    "    df[\"z_global\"] = (df[\"oxygen\"] - df[\"roll_mean\"]) / (df[\"roll_std\"] + cfg.eps)\n",
    "\n",
    "    # Point anomaly score\n",
    "    df[\"score_point\"] = squash_z(df[\"z_global\"], cfg.z_point_low, cfg.z_point_high)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2094f4-cf7f-441f-9577-83ceffeb5a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 4. Collective anomalies\n",
    "# =========================\n",
    "\n",
    "def add_collective_scores(df: pd.DataFrame, cfg: AnomalyConfig) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Collective anomalies: sequences where residuals are consistently large.\n",
    "\n",
    "    We use rolling mean(|z_global|) over a larger window, then scale.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    g = df.groupby(\"sensor_id\", group_keys=False)\n",
    "\n",
    "    df[\"roll_mean_abs_z\"] = g[\"z_global\"].transform(\n",
    "        lambda s: s.abs().rolling(\n",
    "            window=cfg.roll_window_collective,\n",
    "            min_periods=cfg.roll_window_collective // 2\n",
    "        ).mean()\n",
    "    )\n",
    "\n",
    "    df[\"score_collective\"] = (\n",
    "        (df[\"roll_mean_abs_z\"] - cfg.collective_low)\n",
    "        / (cfg.collective_high - cfg.collective_low)\n",
    "    ).clip(lower=0.0, upper=1.0)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27047bf5-4339-48c6-8a3a-8e01292f1491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 5. Contextual anomalies\n",
    "# =========================\n",
    "\n",
    "def add_contextual_scores(df: pd.DataFrame, cfg: AnomalyConfig) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Contextual anomalies: values unusual for their context (e.g. hour-of-day).\n",
    "\n",
    "    Here we build a simple global \"expected\" oxygen per (hour) across all sensors\n",
    "    and compute z-score vs that contextual expectation.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Contextual baseline: global per-hour stats\n",
    "    ctx_stats = (\n",
    "        df.groupby(\"hour\")[\"oxygen\"]\n",
    "        .agg([\"mean\", \"std\"])\n",
    "        .rename(columns={\"mean\": \"ctx_mean_hour\", \"std\": \"ctx_std_hour\"})\n",
    "    )\n",
    "\n",
    "    df = df.join(ctx_stats, on=\"hour\")\n",
    "\n",
    "    # Contextual z-score\n",
    "    df[\"z_context\"] = (df[\"oxygen\"] - df[\"ctx_mean_hour\"]) / (df[\"ctx_std_hour\"] + cfg.eps)\n",
    "\n",
    "    # Contextual anomaly score\n",
    "    df[\"score_context\"] = squash_z(df[\"z_context\"], cfg.z_ctx_low, cfg.z_ctx_high)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4e3cad-63b0-4bae-b52c-24f4a6de2791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 6. Sensor fault anomalies\n",
    "# =========================\n",
    "\n",
    "def add_sensor_fault_scores(df: pd.DataFrame, cfg: AnomalyConfig) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Sensor fault anomalies include:\n",
    "    - Stuck sensor: very low local variance compared to typical variance.\n",
    "    - Spikes/glitches: sharp jumps and immediate reversals.\n",
    "    - High noise: local variance much higher than typical.\n",
    "\n",
    "    We compute three sub-scores and take their max as sensor_fault score.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    g = df.groupby(\"sensor_id\", group_keys=False)\n",
    "\n",
    "    # --- Overall typical stats per sensor for comparison ---\n",
    "    sensor_std = g[\"oxygen\"].transform(\"std\")           # typical volatility\n",
    "    sensor_diff_std = g[\"oxygen\"].transform(lambda s: s.diff().std())\n",
    "\n",
    "    # --- Rolling std over larger window: used for stuck & noise ---\n",
    "    df[\"roll_std_long\"] = g[\"oxygen\"].transform(\n",
    "        lambda s: s.rolling(\n",
    "            window=cfg.roll_window_stuck,\n",
    "            min_periods=cfg.roll_window_stuck // 2\n",
    "        ).std()\n",
    "    )\n",
    "\n",
    "    # 6.1 Stuck sensor score\n",
    "    # If local std is much smaller than typical std, we suspect stuck behaviour.\n",
    "    # Score grows as roll_std_long / sensor_std goes towards 0.\n",
    "    ratio_std = df[\"roll_std_long\"] / (sensor_std + cfg.eps)\n",
    "    # We want large score when ratio_std is very small (< stuck_rel_std_factor)\n",
    "    df[\"score_stuck\"] = (cfg.stuck_rel_std_factor - ratio_std) / cfg.stuck_rel_std_factor\n",
    "    df[\"score_stuck\"] = df[\"score_stuck\"].clip(lower=0.0, upper=1.0)\n",
    "\n",
    "    # 6.2 Spikes / glitches score\n",
    "    # Look for a large jump followed by a reversal.\n",
    "    diff_prev = g[\"oxygen\"].diff()\n",
    "    diff_next = -g[\"oxygen\"].diff(-1)  # difference to next point, negated to compare sign\n",
    "    spike_mag = np.minimum(diff_prev.abs(), diff_next.abs())\n",
    "\n",
    "    # normalized spike magnitude\n",
    "    spike_norm = spike_mag / (sensor_diff_std + cfg.eps)\n",
    "    # candidate spikes: sign reversal + big magnitude\n",
    "    candidate_spike = (diff_prev * diff_next < 0) & (spike_norm > cfg.spike_z_threshold)\n",
    "\n",
    "    df[\"score_spike\"] = 0.0\n",
    "    df.loc[candidate_spike, \"score_spike\"] = (\n",
    "        (spike_norm[candidate_spike] - cfg.spike_z_threshold) / cfg.spike_z_threshold\n",
    "    ).clip(upper=1.0)\n",
    "\n",
    "    # 6.3 High noise score\n",
    "    # If local std is much higher than typical std, we suspect high noise.\n",
    "    df[\"roll_std_noise\"] = g[\"oxygen\"].transform(\n",
    "        lambda s: s.rolling(\n",
    "            window=cfg.roll_window_noise,\n",
    "            min_periods=cfg.roll_window_noise // 2\n",
    "        ).std()\n",
    "    )\n",
    "    noise_ratio = df[\"roll_std_noise\"] / (sensor_std + cfg.eps)\n",
    "    df[\"score_noise\"] = (noise_ratio - 1.0) / (cfg.noise_factor - 1.0)\n",
    "    df[\"score_noise\"] = df[\"score_noise\"].clip(lower=0.0, upper=1.0)\n",
    "\n",
    "    # Combine into a single sensor_fault score\n",
    "    df[\"score_sensor_fault\"] = df[[\"score_stuck\", \"score_spike\", \"score_noise\"]].max(axis=1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c9a52a-3da5-425c-bd48-cd2332b69809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 7. Synthetic anomaly injection (optional, for illustration)\n",
    "# =========================\n",
    "\n",
    "def inject_synthetic_anomalies(\n",
    "    df: pd.DataFrame,\n",
    "    n_point_spikes: int = 20,\n",
    "    n_collective_dips: int = 5,\n",
    "    stuck_length: int = 60,\n",
    "    noise_length: int = 60,\n",
    "    random_state: int = 42,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Injects synthetic anomalies into a copy of df for illustration and testing.\n",
    "\n",
    "    - Random large spikes (point anomalies)\n",
    "    - Random dips over short windows (collective)\n",
    "    - Stuck segments (constant value)\n",
    "    - High-noise segments\n",
    "\n",
    "    Adds a column `synthetic_label` with one of:\n",
    "    - 'normal'\n",
    "    - 'syn_point_spike'\n",
    "    - 'syn_collective_dip'\n",
    "    - 'syn_stuck'\n",
    "    - 'syn_high_noise'\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    df = df.copy()\n",
    "    df[\"synthetic_label\"] = \"normal\"\n",
    "\n",
    "    # Work sensor-by-sensor to keep indexing simple\n",
    "    for sensor_id, df_s in df.groupby(\"sensor_id\"):\n",
    "        idx = df_s.index.to_numpy()\n",
    "        n = len(idx)\n",
    "        if n < 1000:\n",
    "            continue  # skip very short series\n",
    "\n",
    "        # 7.1 Inject point spikes\n",
    "        for _ in range(n_point_spikes):\n",
    "            i = int(rng.integers(0, n))\n",
    "            row_idx = idx[i]\n",
    "            # Add a large positive or negative spike\n",
    "            sign = rng.choice([-1, 1])\n",
    "            magnitude = rng.uniform(20, 40)  # adjust as desired\n",
    "            df.loc[row_idx, \"oxygen\"] += sign * magnitude\n",
    "            df.loc[row_idx, \"synthetic_label\"] = \"syn_point_spike\"\n",
    "\n",
    "        # 7.2 Inject collective dips (sequence anomalies)\n",
    "        for _ in range(n_collective_dips):\n",
    "            start = int(rng.integers(0, n - 120))\n",
    "            length = rng.integers(30, 120)  # 30–120 minutes\n",
    "            segment_idx = idx[start : start + length]\n",
    "            df.loc[segment_idx, \"oxygen\"] *= rng.uniform(0.6, 0.8)  # dip\n",
    "            df.loc[segment_idx, \"synthetic_label\"] = \"syn_collective_dip\"\n",
    "\n",
    "        # 7.3 Inject stuck sensor segments\n",
    "        # pick a start where we have enough space\n",
    "        start = int(rng.integers(0, n - stuck_length))\n",
    "        segment_idx = idx[start : start + stuck_length]\n",
    "        stuck_value = float(df.loc[segment_idx[0], \"oxygen\"])\n",
    "        df.loc[segment_idx, \"oxygen\"] = stuck_value\n",
    "        df.loc[segment_idx, \"synthetic_label\"] = \"syn_stuck\"\n",
    "\n",
    "        # 7.4 Inject high-noise segment\n",
    "        start = int(rng.integers(0, n - noise_length))\n",
    "        segment_idx = idx[start : start + noise_length]\n",
    "        base = df.loc[segment_idx, \"oxygen\"]\n",
    "        df.loc[segment_idx, \"oxygen\"] = base + rng.normal(\n",
    "            0, base.std() * 2.0, size=len(segment_idx)\n",
    "        )\n",
    "        df.loc[segment_idx, \"synthetic_label\"] = \"syn_high_noise\"\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f4069b-f995-46ef-a54a-ed387d8d5d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 8. Combine all scores into severity\n",
    "# =========================\n",
    "\n",
    "def add_severity_score(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Combine all sub-scores into a single anomaly severity score in [0, 1].\n",
    "\n",
    "    We take the max of:\n",
    "    - point\n",
    "    - collective\n",
    "    - contextual\n",
    "    - sensor_fault\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    score_cols = [\n",
    "        \"score_point\",\n",
    "        \"score_collective\",\n",
    "        \"score_context\",\n",
    "        \"score_sensor_fault\",\n",
    "    ]\n",
    "    df[\"severity\"] = df[score_cols].max(axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3541828-0947-4210-8bd5-d2a176c1098e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 9. End-to-end pipeline\n",
    "# =========================\n",
    "\n",
    "\n",
    "def run_anomaly_pipeline(csv_path: str,\n",
    "                         cfg: AnomalyConfig = AnomalyConfig(),\n",
    "                         inject_synthetic: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Full pipeline:\n",
    "    - Load raw data\n",
    "    - Prepare generic sensor frame\n",
    "    - (Optionally) inject synthetic anomalies\n",
    "    - Compute all anomaly scores\n",
    "    - Return scored DataFrame with 1 row per minute per sensor\n",
    "    \"\"\"\n",
    "    # 1) Load\n",
    "    df_raw = load_oxygen_data(csv_path)\n",
    "\n",
    "    # 2) Prepare\n",
    "    df = prepare_sensor_frame(df_raw)\n",
    "\n",
    "    # 3) Inject synthetic anomalies (for illustration / testing)\n",
    "    if inject_synthetic:\n",
    "        df = inject_synthetic_anomalies(df)\n",
    "\n",
    "    # 4) Add anomaly scores\n",
    "    df = add_baseline_and_point_scores(df, cfg)\n",
    "    df = add_collective_scores(df, cfg)\n",
    "    df = add_contextual_scores(df, cfg)\n",
    "    df = add_sensor_fault_scores(df, cfg)\n",
    "    df = add_severity_score(df)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3d529d-318b-41fb-a101-69bc758ff653",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = os.path.abspath(os.path.join(os.getcwd(), \"../\" , \"data\"))\n",
    "DATA_RAW_DIR = Path(os.path.join(DATA_DIR, \"raw\"))\n",
    "# Path to the dataset (adjust this if needed)\n",
    "input_csv = Path(os.path.join(DATA_RAW_DIR, \"oxygen.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fc7744-9553-45ff-b33f-8ea24fca0886",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AnomalyConfig(\n",
    "    roll_window_baseline=60,\n",
    "    roll_window_collective=120,\n",
    "    roll_window_stuck=60,\n",
    "    roll_window_noise=60,\n",
    ")\n",
    "\n",
    "scored = run_anomaly_pipeline(input_csv, cfg=config, inject_synthetic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9ed9fe-bac8-4d09-b8ae-13ea475e7405",
   "metadata": {},
   "outputs": [],
   "source": [
    "scored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a30f16-b87c-4fda-9d8b-6b180a1d4c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scored.to_csv(\"scored_v2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80199d8-b9bf-4b0f-9a16-be119cdaad38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # =========================\n",
    "# # 10. Example usage\n",
    "# # =========================\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Example: run on the provided oxygen.csv\n",
    "#     # (adjust path as needed)\n",
    "#     input_csv = \"oxygen.csv\"   # e.g. \"/mnt/data/oxygen.csv\"\n",
    "\n",
    "#     config = AnomalyConfig(\n",
    "#         roll_window_baseline=60,\n",
    "#         roll_window_collective=120,\n",
    "#         roll_window_stuck=60,\n",
    "#         roll_window_noise=60,\n",
    "#     )\n",
    "\n",
    "#     scored = run_anomaly_pipeline(input_csv, cfg=config, inject_synthetic=True)\n",
    "\n",
    "#     # Peek at the result\n",
    "#     print(scored[[\n",
    "#         \"time\", \"sensor_id\", \"oxygen\",\n",
    "#         \"score_point\", \"score_collective\",\n",
    "#         \"score_context\", \"score_sensor_fault\",\n",
    "#         \"severity\"\n",
    "#     ]].head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
